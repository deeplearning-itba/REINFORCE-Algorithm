{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Muestrar {$\\tau^i$} de $\\pi_{\\theta}(a_t|s_t)$ - Correr M trayectorias usando la policy\n",
    "### 2. Estimar el retorno: $$ R(\\tau_i)  \\approx \\sum_{t=0}^{T}R(s_t^i, a_t^i)$$\n",
    "### 3. Entrenar un modelo: $$ \\nabla_{\\theta} J_{\\theta} \\approx \\frac{1}{M} \\sum_{i=1}^{M}  R(\\tau_i)   \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta}(a_t^i|s_t^i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suponiendo que solo corremos un episodio por iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loss queda (Le ponemos un menos adelante para que tengamos que minimizar):\n",
    "\n",
    "$$ \\huge J_{\\theta} =  - R \\sum_{t=0}^T log \\pi_{\\theta}(a_t|s_t)$$\n",
    "$$ \\huge J_{\\theta} =  - \\sum_{t=0}^T log \\pi_{\\theta}(a_t|s_t) R $$\n",
    "$$ \\huge J_{\\theta} =  \\sum_{t=0}^T log \\frac{1}{\\pi_{\\theta}(a_t|s_t)} R $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando la Entropía cruzada:\n",
    "\n",
    "$$ \\huge H(y_{true}, y_{pred}) = \\sum_{i} y_{true_i} log (\\frac{1}{y_{pred_i}}) $$\n",
    "\n",
    "Ejemplo: \n",
    "\n",
    "- Sumpongamos que tenemos 3 acciones posibles y la red neuronal predijo $y_{pred}$ = [0.2, 0.3, 0.5]\n",
    "- Se muestreó la salida y se eligión la acción 2, es decir la acción con probabilidad 0.3\n",
    "- La $y_{true}$ será [0, 1, 0]\n",
    "\n",
    "$$ \\huge H = 0 log (\\frac{1}{0.2}) + 1 log (\\frac{1}{0.3}) + 0 log (\\frac{1}{0.5}) $$\n",
    "\n",
    "- Si redefinimos la $y_{true}$ como $y_{true}$ = $y_{true} R$\n",
    "- La $y_{true}$ queda [0, R, 0]\n",
    "\n",
    "$$ \\huge H = 0 log (\\frac{1}{0.2}) + R log (\\frac{1}{0.3}) + 0 log (\\frac{1}{0.5}) = R log (\\frac{1}{\\pi_{\\theta}(a_t|s_t)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge tensorboardx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE_helper import BaseAgent, format_as_pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(BaseAgent):\n",
    "    # def __init__(self):\n",
    "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "        ## Defino métrica - loss normalizada (sin el retorno multiplicando)\n",
    "        def loss_metric(y_true, y_pred):\n",
    "            y_true_norm = K.sign(y_true)\n",
    "            return K.categorical_crossentropy(y_true_norm, y_pred)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "        model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "        model.compile(Adam(lr), loss=['categorical_crossentropy'], metrics=[loss_metric])\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, eval=False):\n",
    "        p = self.model.predict([self.observation.reshape(1, self.nS)])\n",
    "        if eval is False:\n",
    "            action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_one_hot = np.zeros(self.nA)\n",
    "        action_one_hot[action] = 1\n",
    "        return action, action_one_hot, p\n",
    "    \n",
    "    def get_entropy(self, preds, epsilon=1e-12):\n",
    "        entropy = np.mean(-np.sum(np.log(preds+epsilon)*preds, axis=1)/np.log(self.nA))\n",
    "        return entropy\n",
    "    \n",
    "    def get_discounted_rewards(self, r):\n",
    "        # Por si es una lista\n",
    "        r = np.array(r, dtype=float)\n",
    "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Ejemplos para probar get_discounted_rewards:\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([1, 1, 1, 1, 1, 1])\n",
    "array([5.98501999, 4.99001   , 3.994004  , 2.997001  , 1.999     ,\n",
    "       1.        ])\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([1, 2, 3, 4, 5, 6])\n",
    "array([20.93010492, 19.95005497, 17.96802299, 14.983006  , 10.994     ,\n",
    "        6.        ])\n",
    "\n",
    "reinforce_agent.get_discounted_rewards([5, 4, -3, -4, 5, 8])\n",
    "array([14.9540949 ,  9.96405896,  5.97002899,  8.979008  , 12.992     ,\n",
    "        8.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent = ReinforceAgent('CartPole-v1', n_experience_episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1\n",
      "action_one_hot: [0. 1.]\n",
      "Policy prob dist: [[0.49976465 0.5002353 ]]\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent.reset_env()\n",
    "action, action_one_hot, p = reinforce_agent.get_action()\n",
    "print('Action:', action)\n",
    "print('action_one_hot:', action_one_hot)\n",
    "print('Policy prob dist:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent.reset_env()\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, last_obs, time_step = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>observation</th>\n",
       "      <th>policy_distribution</th>\n",
       "      <th>sampled_action</th>\n",
       "      <th>rewards</th>\n",
       "      <th>discounted_sum_rewards</th>\n",
       "      <th>episode_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.042, 0.048, 0.006, 0.016]</td>\n",
       "      <td>[0.501, 0.498]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.955</td>\n",
       "      <td>9.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.041, -0.146, 0.006, 0.311]</td>\n",
       "      <td>[0.503, 0.496]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.964</td>\n",
       "      <td>9.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.044, -0.341, 0.012, 0.606]</td>\n",
       "      <td>[0.505, 0.494]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.972</td>\n",
       "      <td>9.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.051, -0.537, 0.025, 0.903]</td>\n",
       "      <td>[0.507, 0.492]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.979</td>\n",
       "      <td>9.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.062, -0.732, 0.043, 1.203]</td>\n",
       "      <td>[0.509, 0.49]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.985</td>\n",
       "      <td>9.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>16</td>\n",
       "      <td>[0.002, 0.553, -0.128, -1.149]</td>\n",
       "      <td>[0.552, 0.447]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.990</td>\n",
       "      <td>19.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>17</td>\n",
       "      <td>[0.013, 0.36, -0.151, -0.899]</td>\n",
       "      <td>[0.539, 0.46]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.994</td>\n",
       "      <td>19.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>18</td>\n",
       "      <td>[0.02, 0.167, -0.169, -0.658]</td>\n",
       "      <td>[0.526, 0.473]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.997</td>\n",
       "      <td>19.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>19</td>\n",
       "      <td>[0.023, 0.364, -0.182, -0.999]</td>\n",
       "      <td>[0.543, 0.456]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.999</td>\n",
       "      <td>19.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20</td>\n",
       "      <td>[0.03, 0.561, -0.202, -1.343]</td>\n",
       "      <td>[0.559, 0.44]</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>19.811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    step                     observation policy_distribution sampled_action  \\\n",
       "0      1   [-0.042, 0.048, 0.006, 0.016]      [0.501, 0.498]         [1, 0]   \n",
       "1      2  [-0.041, -0.146, 0.006, 0.311]      [0.503, 0.496]         [1, 0]   \n",
       "2      3  [-0.044, -0.341, 0.012, 0.606]      [0.505, 0.494]         [1, 0]   \n",
       "3      4  [-0.051, -0.537, 0.025, 0.903]      [0.507, 0.492]         [1, 0]   \n",
       "4      5  [-0.062, -0.732, 0.043, 1.203]       [0.509, 0.49]         [0, 1]   \n",
       "..   ...                             ...                 ...            ...   \n",
       "65    16  [0.002, 0.553, -0.128, -1.149]      [0.552, 0.447]         [1, 0]   \n",
       "66    17   [0.013, 0.36, -0.151, -0.899]       [0.539, 0.46]         [1, 0]   \n",
       "67    18   [0.02, 0.167, -0.169, -0.658]      [0.526, 0.473]         [0, 1]   \n",
       "68    19  [0.023, 0.364, -0.182, -0.999]      [0.543, 0.456]         [0, 1]   \n",
       "69    20   [0.03, 0.561, -0.202, -1.343]       [0.559, 0.44]         [1, 0]   \n",
       "\n",
       "    rewards  discounted_sum_rewards  episode_return  \n",
       "0       1.0                   9.955           9.955  \n",
       "1       1.0                   8.964           9.955  \n",
       "2       1.0                   7.972           9.955  \n",
       "3       1.0                   6.979           9.955  \n",
       "4       1.0                   5.985           9.955  \n",
       "..      ...                     ...             ...  \n",
       "65      1.0                   4.990          19.811  \n",
       "66      1.0                   3.994          19.811  \n",
       "67      1.0                   2.997          19.811  \n",
       "68      1.0                   1.999          19.811  \n",
       "69      1.0                   1.000          19.811  \n",
       "\n",
       "[70 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_as_pandas(time_step, obs, preds, actions, rewards, disc_sum_rews, ep_returns, decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99831235"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.get_entropy(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE_helper import RunningVariance\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas interesantes:\n",
    "- n_experience_episodes=1, epochs=1, lr=0.001\n",
    "- n_experience_episodes=5, epochs=1, lr=0.001\n",
    "- n_experience_episodes=50, epochs=1, lr=0.001\n",
    "- n_experience_episodes=50, epochs=20, lr=0.001\n",
    "- n_experience_episodes=50, epochs=50, lr=0.001\n",
    "- n_experience_episodes=50, epochs=50, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_variance = RunningVariance()\n",
    "running_variance.get_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.5107017617322"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_variance.get_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 51\n",
      "Model on episode 52 improved from -inf to 9.95511979025179. Saved!\n",
      "Episode: 155\n",
      "Model on episode 156 improved from 9.95511979025179 to 9.95511979025179. Saved!\n",
      "Episode: 259\n",
      "Model on episode 260 improved from 9.95511979025179 to 16.86467762617564. Saved!\n",
      "Episode: 311\n",
      "Model on episode 312 improved from 16.86467762617564 to 33.444937900916464. Saved!\n",
      "Episode: 415\n",
      "Model on episode 416 improved from 33.444937900916464 to 84.2794427501054. Saved!\n",
      "Episode: 571"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('CartPole-v1', n_experience_episodes=50, EPISODES=2000, \n",
    "                                 epochs=1, lr=0.004, gamma=0.999, batch_size=512)\n",
    "# reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=1, EPISODES=2000, epochs=1, lr=0.001)\n",
    "\n",
    "running_variance = RunningVariance()\n",
    "initial_time = time()\n",
    "\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, last_obs = reinforce_agent.get_experience_episodes()\n",
    "    for dr in ep_returns:\n",
    "        running_variance.add(dr)\n",
    "        \n",
    "    pseudolabels = actions*ep_returns.reshape(-1, 1)\n",
    "\n",
    "    history = reinforce_agent.model.fit(obs, pseudolabels, verbose=0, epochs=reinforce_agent.epochs, batch_size=128)\n",
    "    \n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      reinforce_agent.get_entropy(preds), \n",
    "                      running_variance.get_variance(), \n",
    "                      history.history['loss_metric'][0], \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]),\n",
    "                      rv_normalized = running_variance.get_variance()/running_variance.get_mean()**2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
